/****************************************************************************
*   Generated by ACUITY 6.21.1
*   Match ovxlib 1.1.30
*
*   Neural Network appliction post-process source file for YuNet Network Binary (.nb format)
****************************************************************************/
/*-------------------------------------------
                Includes
-------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <algorithm>
#include <cmath>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_post_process_yunet_network_binary.h"

#include <opencv2/opencv.hpp>

#define _BASETSD_H

using namespace cv;
using namespace std;

/*-------------------------------------------
                  Variable definitions
-------------------------------------------*/

/*{graph_output_idx, postprocess}*/
const static vsi_nn_postprocess_map_element_t* postprocess_map_yunet = NULL;

/*-------------------------------------------
                  Functions
-------------------------------------------*/

static float* vmx_CopyTensorToBufferYunet
    (
    vsi_nn_graph_t *graph,
    vsi_nn_tensor_t *tensor
    )
{
    vsi_status status = VSI_FAILURE;
    vsi_size_t i, sz, stride;
    float *buffer = NULL;
    uint8_t *tensor_data = NULL;

    sz = 1;
    for(i = 0; i < tensor->attr.dim_num; i++)
    {
        sz *= tensor->attr.size[i];
    }

    stride = (vsi_size_t)vsi_nn_TypeGetBytes(tensor->attr.dtype.vx_type);
    if(stride == 0)
    {
        stride = 1;
    }
    tensor_data = (uint8_t *)vsi_nn_ConvertTensorToData(graph, tensor);
    buffer = (float *)malloc(sizeof(float) * sz);

    for(i = 0; i < sz; i++)
    {
        status = vsi_nn_DtypeToFloat32(&tensor_data[stride * i], &buffer[i], &tensor->attr.dtype);
    }
    
    if(tensor_data) vsi_nn_Free(tensor_data);
    
    return buffer;
}

vsi_status vnn_PostProcessYunet
    (
    vsi_nn_graph_t *graph,
    std::vector<YunetFaceObject>& faces,
    int inputW,
    int inputH
    )
{
    vsi_status status = VSI_FAILURE;
    vsi_nn_tensor_t *tensor;
    float *data = NULL;
    std::vector<float*> output_blobs;
    uint32_t i;

    printf("[YuNet PostProcess] Starting...\n");

    // Extract all 12 output tensors from Yunet model
    for(i = 0; i < graph->output.num; i++)
    {
        tensor = vsi_nn_GetTensor(graph, graph->output.tensors[i]);

        if (tensor) {
            data = vmx_CopyTensorToBufferYunet(graph, tensor);
            output_blobs.push_back(data);
        }
    }
    printf("[YuNet PostProcess] Extracted %ld output blobs\n", output_blobs.size());

    // Yunet detection processing
    const int divisor = 32;
    int topK = 5000;
    float scoreThreshold = 0.6f;  // Lower threshold for better detection
    float nmsThreshold = 0.3f;
    const std::vector<int> strides = {8, 16, 32};

    int padW = (int((inputW - 1) / divisor) + 1) * divisor;
    int padH = (int((inputH - 1) / divisor) + 1) * divisor;

    cv::Mat detected_faces;
    
    // Process each stride level
    for (size_t i = 0; i < strides.size(); ++i) {
        int cols = int(padW / strides[i]);
        int rows = int(padH / strides[i]);

        float* cls_v = output_blobs[i];                       // Classification scores
        float* obj_v = output_blobs[i + strides.size() * 1];  // Objectness scores
        float* bbox_v = output_blobs[i + strides.size() * 2]; // Bounding boxes
        float* kps_v = output_blobs[i + strides.size() * 3];  // Landmarks (keypoints)

        // (tl_x, tl_y, w, h, re_x, re_y, le_x, le_y, nt_x, nt_y, rcm_x, rcm_y, lcm_x, lcm_y, score)
        // 'tl': top left point of the bounding box
        // 're': right eye, 'le': left eye
        // 'nt': nose tip
        // 'rcm': right corner of mouth, 'lcm': left corner of mouth
        Mat face(1, 15, CV_32FC1);

        for(int r = 0; r < rows; ++r) {
            for(int c = 0; c < cols; ++c) {
                size_t idx = r * cols + c;

                // Get score
                float cls_score = cls_v[idx];
                float obj_score = obj_v[idx];

                // Clamp scores
                cls_score = std::min(cls_score, 1.f);
                cls_score = std::max(cls_score, 0.f);
                obj_score = std::min(obj_score, 1.f);
                obj_score = std::max(obj_score, 0.f);
                float score = std::sqrt(cls_score * obj_score);
                face.at<float>(0, 14) = score;

                // Checking if the score meets the threshold before adding the face
                if (score < scoreThreshold)
                    continue;
                
                // Get bounding box
                float cx = ((c + bbox_v[idx * 4 + 0]) * strides[i]);
                float cy = ((r + bbox_v[idx * 4 + 1]) * strides[i]);
                float w = exp(bbox_v[idx * 4 + 2]) * strides[i];
                float h = exp(bbox_v[idx * 4 + 3]) * strides[i];

                float x1 = cx - w / 2.f;
                float y1 = cy - h / 2.f;

                face.at<float>(0, 0) = x1;
                face.at<float>(0, 1) = y1;
                face.at<float>(0, 2) = w;
                face.at<float>(0, 3) = h;

                // Get landmarks (5 points: 2 eyes, nose, 2 mouth corners)
                for(int n = 0; n < 5; ++n) {
                    face.at<float>(0, 4 + 2 * n) = (kps_v[idx * 10 + 2 * n] + c) * strides[i];
                    face.at<float>(0, 4 + 2 * n + 1) = (kps_v[idx * 10 + 2 * n + 1] + r) * strides[i];
                }
                detected_faces.push_back(face);
            }
        }
    }
    
    printf("[YuNet PostProcess] Detected %d faces before NMS\n", detected_faces.rows);

    // Apply Non-Maximum Suppression
    Mat nms_faces;
    if (detected_faces.rows > 1)
    {
        // Retrieve boxes and scores
        std::vector<Rect2i> faceBoxes;
        std::vector<float> faceScores;
        for (int rIdx = 0; rIdx < detected_faces.rows; rIdx++)
        {
            faceBoxes.push_back(Rect2i(int(detected_faces.at<float>(rIdx, 0)),
                                        int(detected_faces.at<float>(rIdx, 1)),
                                        int(detected_faces.at<float>(rIdx, 2)),
                                        int(detected_faces.at<float>(rIdx, 3))));
            faceScores.push_back(detected_faces.at<float>(rIdx, 14));
        }

        std::vector<int> keepIdx;
        dnn::NMSBoxes(faceBoxes, faceScores, scoreThreshold, nmsThreshold, keepIdx, 1.f, topK);

        // Get NMS results
        for (int idx: keepIdx)
        {
            nms_faces.push_back(detected_faces.row(idx));
        }
    }
    else if (detected_faces.rows == 1)
    {
        nms_faces = detected_faces;
    }

    printf("[YuNet PostProcess] Detected %d faces after NMS\n", nms_faces.rows);

    // Convert to YunetFaceObject structure
    faces.clear();
    for (int i = 0; i < nms_faces.rows; ++i)
    {
        YunetFaceObject faceObj;
        
        // Bounding box
        int x1 = static_cast<int>(nms_faces.at<float>(i, 0));
        int y1 = static_cast<int>(nms_faces.at<float>(i, 1));
        int w = static_cast<int>(nms_faces.at<float>(i, 2));
        int h = static_cast<int>(nms_faces.at<float>(i, 3));
        faceObj.rect = cv::Rect(x1, y1, w, h);
        
        // Landmarks (5 points x 2 coordinates = 10 values)
        for (int j = 0; j < 5; ++j)
        {
            faceObj.landmarks[2*j] = nms_faces.at<float>(i, 4 + 2*j);     // x coordinate
            faceObj.landmarks[2*j + 1] = nms_faces.at<float>(i, 4 + 2*j + 1); // y coordinate
        }
        
        // Confidence
        faceObj.confidence = nms_faces.at<float>(i, 14);
        
        faces.push_back(faceObj);
        
        printf("[YuNet] Face %d: bbox=(%d,%d,%d,%d), conf=%.4f, landmarks=(%.1f,%.1f),(%.1f,%.1f),(%.1f,%.1f),(%.1f,%.1f),(%.1f,%.1f)\n", 
               i, x1, y1, w, h, faceObj.confidence,
               faceObj.landmarks[0], faceObj.landmarks[1],
               faceObj.landmarks[2], faceObj.landmarks[3],
               faceObj.landmarks[4], faceObj.landmarks[5],
               faceObj.landmarks[6], faceObj.landmarks[7],
               faceObj.landmarks[8], faceObj.landmarks[9]);
    }

    // Free output blobs
    for(size_t i = 0; i < output_blobs.size(); i++)
    {
        if(output_blobs[i]) free(output_blobs[i]);
    }

    status = VSI_SUCCESS;
    return status;
}

const vsi_nn_postprocess_map_element_t * vnn_GetPostProcessMapYunet()
{
    return postprocess_map_yunet;
}

uint32_t vnn_GetPostProcessMapCountYunet()
{
    if (postprocess_map_yunet == NULL)
       return 0;
    else
        return sizeof(postprocess_map_yunet) / sizeof(vsi_nn_postprocess_map_element_t);
}
