/****************************************************************************
*   Generated by ACUITY 6.21.1
*   Match ovxlib 1.1.53
*
*   Neural Network application project entry file
****************************************************************************/
/*-------------------------------------------
                Includes
-------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#ifdef __linux__
#include <time.h>
#include <inttypes.h>
#elif defined(_WIN32)
#include <windows.h>
#include <time.h>
#endif

#define _BASETSD_H

#include "vsi_nn_pub.h"

#include <opencv2/opencv.hpp>
#include <fstream>
#include <vector>
#include <map>
#include <string>
#include <algorithm>
#include <numeric>
#include <ctime>
#include <iostream>

#include "vnn_global.h"
#include "vnn_pre_process.h"
#include "vnn_post_process_sface.h"
// OLD FACE DETECTOR - Commented out for Yunet migration
// #include "vnn_post_process_facedetector.hpp"
#include "vnn_post_process_anfispoof.h"

// NEW YUNET FACE DETECTOR - Network Binary (.nb format)
#include "vnn_pre_process_yunet_network_binary.h"
#include "vnn_post_process_yunet_network_binary.h"

extern "C" {
// OLD FACE DETECTOR - Commented out
// #include "vnn_facedetector3uint8.h"
// NEW YUNET FACE DETECTOR - Network Binary (.nb format)
#include "vnn_yunet_network_binary.h"
// #include "vnn_sface.h"  // Old format: sface.export.data
#include "vnn_sface_network_binary.h"  // New format: sface.nb (optimized NBG binary)
#include "vnn_antispoof.h"
}

#include "FaceRecog_wrapper.h"

/*-------------------------------------------
        Macros and Variables
-------------------------------------------*/
#ifdef __linux__
#define VSI_UINT64_SPECIFIER PRIu64
#elif defined(_WIN32)
#define VSI_UINT64_SPECIFIER "I64u"
#endif

#define VMX_SW_VERSION "2025JULY21.001"

/*-------------------------------------------
    Frame Processing & Access Control
-------------------------------------------*/
// Frame skip control - process every 4th frame instead of every 2nd frame
static int frame_counter = 0;
#define FRAME_SKIP_INTERVAL 4  // Changed from 2 to 4

// Access time tracking - minimum 5 minutes between access logs
static std::map<std::string, time_t> last_access_times;
#define MIN_ACCESS_INTERVAL_SECONDS (5 * 60)  // 5 minutes in seconds

// --- Helper functions and metadata for preprocessing (copied from vnn_pre_process_yunet_network_binary.cpp) ---
#define VNN_PREPRO_NONE -1
#define VNN_PREPRO_REORDER 0
#define VNN_PREPRO_MEAN 1
#define VNN_PREPRO_SCALE 2
#define VNN_PREPRO_NUM 3 // Total number of preprocess steps

// TYPEDEFS REMOVED TO PREVENT MULTIPLE DEFINITION ERRORS
// They are already included from vnn_pre_process.h

#define INPUT_META_NUM 1
static vnn_input_meta_t input_meta_tab_yunet[INPUT_META_NUM];
static void _load_input_meta_yunet()
{
    uint32_t i;
    for (i = 0; i < INPUT_META_NUM; i++)
    {
        memset(&input_meta_tab_yunet[i].image.preprocess,
            VNN_PREPRO_NONE, sizeof(int32_t) * VNN_PREPRO_NUM);
    }
    /* lid: input_118 - Yunet expects input size [640, 640, 3] */
    // ===== YUNET RGB INPUT MODE (RESTORED - Original working version) =====
    input_meta_tab_yunet[0].image.preprocess[0] = VNN_PREPRO_REORDER;  // BGR to RGB conversion
    input_meta_tab_yunet[0].image.preprocess[1] = VNN_PREPRO_MEAN;
    input_meta_tab_yunet[0].image.preprocess[2] = VNN_PREPRO_SCALE;
    
    input_meta_tab_yunet[0].image.reorder[0] = 2;  // BGR to RGB reorder
    input_meta_tab_yunet[0].image.reorder[1] = 1;
    input_meta_tab_yunet[0].image.reorder[2] = 0;
    
    input_meta_tab_yunet[0].image.mean[0] = 0;
    input_meta_tab_yunet[0].image.mean[1] = 0;
    input_meta_tab_yunet[0].image.mean[2] = 0;
    input_meta_tab_yunet[0].image.scale[0] = 1.0;
    input_meta_tab_yunet[0].image.scale[1] = 1.0;
    input_meta_tab_yunet[0].image.scale[2] = 1.0;
}

static float *_imageData_to_float32
    (
    uint8_t *bmpData,
    vsi_nn_tensor_t *tensor
    )
{
    float *fdata;
    vsi_size_t sz,i;

    fdata = NULL;
    sz = vsi_nn_GetElementNum(tensor);
    fdata = (float *)malloc(sz * sizeof(float));
    // TEST_CHECK_PTR(fdata, final); // Cannot use TEST_CHECK_PTR here, needs to be handled by caller

    if (!fdata) return NULL; // Manual check

    for(i = 0; i < sz; i++)
    {
        fdata[i] = (float)bmpData[i];
    }

    // final: // Label not defined
    return fdata;
}

static void _data_scale
    (
    float *fdata,
    vnn_input_meta_t *meta,
    vsi_nn_tensor_t *tensor
    )
{
    vsi_size_t s0,s1,s2;
    vsi_size_t i,j,offset;
    float val,scale;

    s0 = tensor->attr.size[0];
    s1 = tensor->attr.size[1];
    s2 = tensor->attr.size[2];
    for(i = 0; i < s2; i++)
    {
        offset = s0 * s1 * i;
        scale = meta->image.scale[i];
        for(j = 0; j < s0 * s1; j++)
        {
            val = fdata[offset + j] * scale;
            fdata[offset + j ] = val;
        }
    }

}

static void _data_mean
    (
    float *fdata,
    vnn_input_meta_t *meta,
    vsi_nn_tensor_t *tensor
    )
{
    vsi_size_t s0,s1,s2;
    vsi_size_t i,j,offset;
    float val,mean;

    s0 = tensor->attr.size[0];
    s1 = tensor->attr.size[1];
    s2 = tensor->attr.size[2];

    for(i = 0; i < s2; i++)
    {
        offset = s0 * s1 * i;
        mean = meta->image.mean[i];
        for(j = 0; j < s0 * s1; j++)
        {
            val = fdata[offset + j] - mean;
            fdata[offset + j ] = val;
        }
    }

}

static void _data_transform
    (
    float *fdata,
    vnn_input_meta_t *meta,
    vsi_nn_tensor_t *tensor
    )
{
    vsi_size_t s0,s1,s2;
    vsi_size_t i,j,offset,sz,order;
    float * data;
    uint32_t * reorder;

    data = NULL;
    reorder = meta->image.reorder;
    s0 = tensor->attr.size[0];
    s1 = tensor->attr.size[1];
    s2 = tensor->attr.size[2];
    sz = vsi_nn_GetElementNum(tensor);
    data = (float *)malloc(sz * sizeof(float));
    // TEST_CHECK_PTR(data, final); // Cannot use TEST_CHECK_PTR here, needs to be handled by caller
    if (!data) return; // Manual check
    memset(data, 0, sizeof(float) * sz);

    for(i = 0; i < s2; i++)
    {
        if(s2 > 1 && reorder[i] <= s2)
        {
            order = reorder[i];
        }
        else
        {
            order = i;
        }

        offset = s0 * s1 * i;
        for(j = 0; j < s0 * s1; j++)
        {
            data[j + offset] = fdata[j * s2 + order];
        }
    }

    memcpy(fdata, data, sz * sizeof(float));
    // final: // Label not defined
    if(data)free(data);
}

static uint8_t *_float32_to_dtype
    (
    float *fdata,
    vsi_nn_tensor_t *tensor
    )
{
    vsi_status status;
    uint8_t *data;
    vsi_size_t sz,i,stride;

    sz = vsi_nn_GetElementNum(tensor);
    stride = vsi_nn_TypeGetBytes(tensor->attr.dtype.vx_type);
    if(stride == 0)
    {
        stride = 1;
    }
    data = (uint8_t *)malloc(stride * sz * sizeof(uint8_t));
    // TEST_CHECK_PTR(data, final); // Cannot use TEST_CHECK_PTR here, needs to be handled by caller
    if (!data) return NULL; // Manual check
    memset(data, 0, stride * sz * sizeof(uint8_t));

    for(i = 0; i < sz; i++)
    {
        status = vsi_nn_Float32ToDtype(fdata[i], &data[stride * i], &tensor->attr.dtype);
        if(status != VSI_SUCCESS)
        {
            if(data)free(data);
            return NULL;
        }
    }

    // final: // Label not defined
    return data;
}
// --- End of Helper functions and metadata for preprocessing ---

/*-------------------------------------------
                  Prototype Functions
-------------------------------------------*/

float checkSpoofing(cv::Mat img);
std::vector<float> extractFeatures(cv::Mat img);

/*-------------------------------------------
    Access Logging Functions
-------------------------------------------*/
// Get current timestamp as string
std::string getCurrentTimestamp() {
    time_t now = time(0);
    char timestamp[100];
    struct tm* tm_info = localtime(&now);
    strftime(timestamp, sizeof(timestamp), "%Y-%m-%d %H:%M:%S", tm_info);
    return std::string(timestamp);
}

// Check if enough time has passed since last access for this user
bool shouldLogAccess(const std::string& username) {
    time_t current_time = time(0);
    
    auto it = last_access_times.find(username);
    if (it == last_access_times.end()) {
        // First time access for this user
        last_access_times[username] = current_time;
        return true;
    }
    
    // Check if enough time has passed
    time_t time_diff = current_time - it->second;
    if (time_diff >= MIN_ACCESS_INTERVAL_SECONDS) {
        last_access_times[username] = current_time;
        return true;
    }
    
    return false;
}

// Log access to file
void logAccess(const std::string& username, float similarity) {
    std::string timestamp = getCurrentTimestamp();
    std::string log_message = timestamp + " - User: " + username + 
                             " (Similarity: " + std::to_string(similarity) + ")\n";
    
    // Log to console
    printf("üö™ ACCESS LOG: %s", log_message.c_str());
    
    // Log to file - you can change the path here
    std::string log_file_path = "access_log.txt";  // Default: current directory
    // Alternative paths:
    // std::string log_file_path = "/path/to/logs/access_log.txt";  // Linux
    // std::string log_file_path = "C:\\logs\\access_log.txt";     // Windows
    
    std::ofstream access_log(log_file_path, std::ios::app);
    if (access_log.is_open()) {
        access_log << log_message;
        access_log.close();
    }
}

// Check frame skip - only process every FRAME_SKIP_INTERVAL frames
bool shouldProcessFrame() {
    frame_counter++;
    if (frame_counter >= FRAME_SKIP_INTERVAL) {
        frame_counter = 0;
        return true;
    }
    return false;
}
/*-------------------------------------------
                  Functions
-------------------------------------------*/

#if 1
namespace cv
{

cv::Mat getSimilarityTransformMatrix(float src[5][2]) {
    float dst[5][2] = { {38.2946f, 51.6963f}, {73.5318f, 51.5014f}, {56.0252f, 71.7366f}, {41.5493f, 92.3655f}, {70.7299f, 92.2041f} };
    float avg0 = (src[0][0] + src[1][0] + src[2][0] + src[3][0] + src[4][0]) / 5;
    float avg1 = (src[0][1] + src[1][1] + src[2][1] + src[3][1] + src[4][1]) / 5;
    //Compute mean of src and dst.
    float src_mean[2] = { avg0, avg1 };
    float dst_mean[2] = { 56.0262f, 71.9008f };
    //Subtract mean from src and dst.
    float src_demean[5][2];
    for (int i = 0; i < 2; i++)
    {
        for (int j = 0; j < 5; j++)
        {
            src_demean[j][i] = src[j][i] - src_mean[i];
        }
    }
    float dst_demean[5][2];
    for (int i = 0; i < 2; i++)
    {
        for (int j = 0; j < 5; j++)
        {
            dst_demean[j][i] = dst[j][i] - dst_mean[i];
        }
    }
    double A00 = 0.0, A01 = 0.0, A10 = 0.0, A11 = 0.0;
    for (int i = 0; i < 5; i++)
        A00 += dst_demean[i][0] * src_demean[i][0];
    A00 = A00 / 5;
    for (int i = 0; i < 5; i++)
        A01 += dst_demean[i][0] * src_demean[i][1];
    A01 = A01 / 5;
    for (int i = 0; i < 5; i++)
        A10 += dst_demean[i][1] * src_demean[i][0];
    A10 = A10 / 5;
    for (int i = 0; i < 5; i++)
        A11 += dst_demean[i][1] * src_demean[i][1];
    A11 = A11 / 5;
    cv::Mat A = (cv::Mat_<double>(2, 2) << A00, A01, A10, A11);
    double d[2] = { 1.0, 1.0 };
    double detA = A00 * A11 - A01 * A10;
    if (detA < 0)
        d[1] = -1;
    double T[3][3] = { {1.0, 0.0, 0.0}, {0.0, 1.0, 0.0}, {0.0, 0.0, 1.0} };
    cv::Mat s, u, vt, v;
    SVD::compute(A, s, u, vt);
    double smax = s.ptr<double>(0)[0]>s.ptr<double>(1)[0] ? s.ptr<double>(0)[0] : s.ptr<double>(1)[0];
    double tol = smax * 2 * FLT_MIN;
    int rank = 0;
    if (s.ptr<double>(0)[0]>tol)
        rank += 1;
    if (s.ptr<double>(1)[0]>tol)
        rank += 1;
    double arr_u[2][2] = { {u.ptr<double>(0)[0], u.ptr<double>(0)[1]}, {u.ptr<double>(1)[0], u.ptr<double>(1)[1]} };
    double arr_vt[2][2] = { {vt.ptr<double>(0)[0], vt.ptr<double>(0)[1]}, {vt.ptr<double>(1)[0], vt.ptr<double>(1)[1]} };
    double det_u = arr_u[0][0] * arr_u[1][1] - arr_u[0][1] * arr_u[1][0];
    double det_vt = arr_vt[0][0] * arr_vt[1][1] - arr_vt[0][1] * arr_vt[1][0];
    if (rank == 1)
    {
        if ((det_u*det_vt) > 0)
        {
            Mat uvt = u*vt;
            T[0][0] = uvt.ptr<double>(0)[0];
            T[0][1] = uvt.ptr<double>(0)[1];
            T[1][0] = uvt.ptr<double>(1)[0];
            T[1][1] = uvt.ptr<double>(1)[1];
        }
        else
        {
            double temp = d[1];
            d[1] = -1;
            Mat D = (Mat_<double>(2, 2) << d[0], 0.0, 0.0, d[1]);
            Mat Dvt = D*vt;
            Mat uDvt = u*Dvt;
            T[0][0] = uDvt.ptr<double>(0)[0];
            T[0][1] = uDvt.ptr<double>(0)[1];
            T[1][0] = uDvt.ptr<double>(1)[0];
            T[1][1] = uDvt.ptr<double>(1)[1];
            d[1] = temp;
        }
    }
    else
    {
        Mat D = (Mat_<double>(2, 2) << d[0], 0.0, 0.0, d[1]);
        Mat Dvt = D*vt;
        Mat uDvt = u*Dvt;
        T[0][0] = uDvt.ptr<double>(0)[0];
        T[0][1] = uDvt.ptr<double>(0)[1];
        T[1][0] = uDvt.ptr<double>(1)[0];
        T[1][1] = uDvt.ptr<double>(1)[1];
    }
    double var1 = 0.0;
    for (int i = 0; i < 5; i++)
        var1 += src_demean[i][0] * src_demean[i][0];
    var1 = var1 / 5;
    double var2 = 0.0;
    for (int i = 0; i < 5; i++)
        var2 += src_demean[i][1] * src_demean[i][1];
    var2 = var2 / 5;
    double scale = 1.0 / (var1 + var2)* (s.ptr<double>(0)[0] * d[0] + s.ptr<double>(1)[0] * d[1]);
    double TS[2];
    TS[0] = T[0][0] * src_mean[0] + T[0][1] * src_mean[1];
    TS[1] = T[1][0] * src_mean[0] + T[1][1] * src_mean[1];
    T[0][2] = dst_mean[0] - scale*TS[0];
    T[1][2] = dst_mean[1] - scale*TS[1];
    T[0][0] *= scale;
    T[0][1] *= scale;
    T[1][0] *= scale;
    T[1][1] *= scale;
    Mat transform_mat = (Mat_<double>(2, 3) << T[0][0], T[0][1], T[0][2], T[1][0], T[1][1], T[1][2]);
    return transform_mat;
}

void alignCrop(InputArray _src_img, InputArray _face_mat, OutputArray _aligned_img)
{
    Mat face_mat = _face_mat.getMat();
    float src_point[5][2];
    for (int row = 0; row < 5; ++row)
    {
        for(int col = 0; col < 2; ++col)
        {
            src_point[row][col] = face_mat.at<float>(0, row*2+col+4);
        }
    }
    Mat warp_mat = getSimilarityTransformMatrix(src_point);
    warpAffine(_src_img, _aligned_img, warp_mat, Size(112, 112), INTER_LINEAR);
}
}   //namespace cv
#endif
#if ANTI_SPOOFING
static void vnn_ReleaseNeuralNetworkAntiSpoof
    (
    vsi_nn_graph_t *graph
    )
{
    vnn_ReleaseAntiSpoof( graph, TRUE );
    if (vnn_UseImagePreprocessNode())
    {
        vnn_ReleaseBufferImage();
    }
}
#endif
static void vnn_ReleaseNeuralNetworkSface
    (
    vsi_nn_graph_t *graph
    )
{
    // vnn_ReleaseSface( graph, TRUE );  // Old format
    vnn_ReleaseSfaceNBG( graph, TRUE );  // New NBG format
    if (vnn_UseImagePreprocessNode())
    {
        vnn_ReleaseBufferImage();
    }
}
#if ANTI_SPOOFING
static float vnn_PostProcessNeuralNetworkAntiSpoof
    (
    vsi_nn_graph_t *graph
    )
{
    return vnn_PostProcessAntiSpoof( graph );
}
#endif
static vsi_status vnn_PostProcessNeuralNetworkSface
    (
    vsi_nn_graph_t *graph
    )
{
    return vnn_PostProcessSface( graph );
}

#define BILLION                                 1000000000
static uint64_t get_perf_count()
{
#if defined(__linux__) || defined(__ANDROID__) || defined(__QNX__) || defined(__CYGWIN__)
    struct timespec ts;

    clock_gettime(CLOCK_MONOTONIC, &ts);

    return (uint64_t)((uint64_t)ts.tv_nsec + (uint64_t)ts.tv_sec * BILLION);
#elif defined(_WIN32) || defined(UNDER_CE)
    LARGE_INTEGER freq;
    LARGE_INTEGER ln;

    QueryPerformanceFrequency(&freq);
    QueryPerformanceCounter(&ln);

    return (uint64_t)(ln.QuadPart * BILLION / freq.QuadPart);
#endif
}

static vsi_status vnn_VerifyGraph
    (
    vsi_nn_graph_t *graph
    )
{
    vsi_status status = VSI_FAILURE;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    /* Verify graph */
    printf("Verify...\n");
    tmsStart = get_perf_count();
    status = vsi_nn_VerifyGraph( graph );
    TEST_CHECK_STATUS(status, final);
    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("Verify Graph: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return status;
}

static vsi_status vnn_ProcessGraph
    (
    vsi_nn_graph_t *graph
    )
{
    vsi_status status = VSI_FAILURE;
    int32_t i,loop;
    char *loop_s;
    // uint64_t tmsStart, tmsEnd, sigStart, sigEnd; // Removed to clean up unused variable warnings
    // float msVal, usVal; // Removed to clean up unused variable warnings

    status = VSI_FAILURE;
    loop = 1; /* default loop time is 1 */
    loop_s = getenv("VNN_LOOP_TIME");
    if(loop_s)
    {
        loop = atoi(loop_s);
    }

    /* Run graph */
    // tmsStart = get_perf_count();
    // printf("Start run graph [%d] times...\n", loop);
    for(i = 0; i < loop; i++)
    {
        // sigStart = get_perf_count();
#ifdef VNN_APP_ASYNC_RUN
        status = vsi_nn_AsyncRunGraph( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Async Run graph the %d time fail\n", i);
        }
        TEST_CHECK_STATUS( status, final );

        //do something here...

        status = vsi_nn_AsyncRunWait( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Wait graph the %d time fail\n", i);
        }
#else
        status = vsi_nn_RunGraph( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Run graph the %d time fail\n", i);
        }
#endif
        TEST_CHECK_STATUS( status, final );

        // sigEnd = get_perf_count();
        // msVal = (sigEnd - sigStart)/(float)1000000;
        // usVal = (sigEnd - sigStart)/(float)1000;
        // printf("Run the %u time: %.2fms or %.2fus\n", (i + 1), msVal, usVal);
    }
    // tmsEnd = get_perf_count();
    // msVal = (tmsEnd - tmsStart)/(float)1000000;
    // usVal = (tmsEnd - tmsStart)/(float)1000;
    // printf("vxProcessGraph execution time:\n");
    // printf("Total   %.2fms or %.2fus\n", msVal, usVal);
    // printf("Average %.2fms or %.2fus\n", ((float)usVal)/1000/loop, ((float)usVal)/loop);

final:
    return status;
}

static vsi_status vnn_PreProcessNeuralNetwork
    (
    vsi_nn_graph_t *graph,
    uint32_t input_num,
    uint8_t *input_buf
    )
{
    vsi_status status = VSI_FAILURE;
    vsi_nn_tensor_t *tensor;

    if(input_num != graph->input.num)
    {
        printf("Graph need %u inputs, but enter %u inputs!!!\n",
               graph->input.num, input_num);
        return status;
    }

    tensor = vsi_nn_GetTensor( graph, graph->input.tensors[0] );
    
    /* Copy the Pre-processed data to input tensor */
    status = vsi_nn_CopyDataToTensor(graph, tensor, input_buf);
    TEST_CHECK_STATUS(status, final);

    status = VSI_SUCCESS;
final:
    return status;
}
#if 1
// Add by NhanTran
// OLD FACE DETECTOR - Commented out for Yunet migration
/*
static vsi_nn_graph_t *vnn_CreateNeuralNetworkFacedetector3Uint8
    (
    const char *data_file_name
    )
{
    vsi_nn_graph_t *graph = NULL;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    tmsStart = get_perf_count();
    graph = vnn_CreateFacedetector3Uint8( data_file_name, NULL,
                      vnn_GetPreProcessMap(), vnn_GetPreProcessMapCount(),
                      vnn_GetPostProcessMap(), vnn_GetPostProcessMapCount() );
    TEST_CHECK_PTR(graph, final);

    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("%s():%d\n", __FUNCTION__, __LINE__);
    printf("Create Neural Network: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return graph;
}

static void vnn_ReleaseNeuralNetworkFacedetector3Uint8
    (
    vsi_nn_graph_t *graph
    )
{
    vnn_ReleaseFacedetector3Uint8( graph, TRUE );
    if (vnn_UseImagePreprocessNode())
    {
        vnn_ReleaseBufferImage();
    }
}
*/

// NEW YUNET FACE DETECTOR
static vsi_nn_graph_t *vnn_CreateNeuralNetworkYunet
    (
    const char *data_file_name
    )
{
    vsi_nn_graph_t *graph = NULL;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    tmsStart = get_perf_count();
    graph = vnn_CreateYunetNBG( data_file_name, NULL,
                      vnn_GetPreProcessMapYunet(), vnn_GetPreProcessMapCountYunet(),
                      vnn_GetPostProcessMapYunet(), vnn_GetPostProcessMapCountYunet() );
    TEST_CHECK_PTR(graph, final);

    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("[YUNET] %s():%d\n", __FUNCTION__, __LINE__);
    printf("[YUNET] Create Neural Network: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return graph;
}

static void vnn_ReleaseNeuralNetworkYunet
    (
    vsi_nn_graph_t *graph
    )
{
    vnn_ReleaseYunetNBG( graph, TRUE );
    if (vnn_UseImagePreprocessNodeYunet())
    {
        vnn_ReleaseBufferImageYunet();
    }
}

// OLD FACE DETECTOR - Commented out
/*
static vsi_status vnn_PostProcessNeuralNetworkFaceDetector3Uint8
    (
    vsi_nn_graph_t *graph,
    std::vector<FaceObject>& faces
    )
{
    return vnn_PostProcessFacedetector3Uint8( graph, faces );
}
*/

// NEW YUNET FACE DETECTOR
static vsi_status vnn_PostProcessNeuralNetworkYunet
    (
    vsi_nn_graph_t *graph,
    std::vector<YunetFaceObject>& faces,
    int inputW,
    int inputH
    )
{
    return vnn_PostProcessYunet( graph, faces, inputW, inputH );
}
#if ANTI_SPOOFING
static vsi_nn_graph_t *vnn_CreateNeuralNetworkAntiSpoof
    (
    const char *data_file_name
    )
{
    vsi_nn_graph_t *graph = NULL;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    tmsStart = get_perf_count();
    graph = vnn_CreateAntiSpoof( data_file_name, NULL,
                      vnn_GetPreProcessMap(), vnn_GetPreProcessMapCount(),
                      vnn_GetPostProcessMap(), vnn_GetPostProcessMapCount() );
    TEST_CHECK_PTR(graph, final);

    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("Create Neural Network: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return graph;
}
#endif
#endif

static vsi_nn_graph_t *vnn_CreateNeuralNetworkSface
    (
    const char *data_file_name
    )
{
    vsi_nn_graph_t *graph = NULL;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    tmsStart = get_perf_count();
    // graph = vnn_CreateSface( data_file_name, NULL,  // Old format
    graph = vnn_CreateSfaceNBG( data_file_name, NULL,  // New NBG format
                      vnn_GetPreProcessMap(), vnn_GetPreProcessMapCount(),
                      vnn_GetPostProcessMap(), vnn_GetPostProcessMapCount() );
    TEST_CHECK_PTR(graph, final);

    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("Create Neural Network: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return graph;
}


uint8_t* pre_process_get_buffer_from_mat(cv::Mat matImg, uint32_t *buffer_size) {
    uint8_t *data_buffer = NULL;
    if (!matImg.empty()) {
        uint32_t buff_size = matImg.cols * matImg.rows * sizeof(uint8_t)*matImg.channels();

        data_buffer = (uint8_t*)malloc(buff_size);

        if (data_buffer) {
            memcpy(data_buffer, matImg.data, buff_size);
            // *buffer_size = buff_size;
        } else
            return nullptr;
    }
    // printf("[%s()]::[%d] buff_size = %d\n", __FUNCTION__, __LINE__, buff_size);
    return data_buffer;
}

#if 1
// Add by NhanTran
vsi_status statusFaceDetect = VSI_FAILURE;
vsi_status statusSface = VSI_FAILURE;
#endif
#if ANTI_SPOOFING
vsi_status statusAntiSpf = VSI_FAILURE;
#endif
vsi_nn_graph_t* graphFaceDetect = nullptr;
vsi_nn_graph_t* graphSface = nullptr;
#if ANTI_SPOOFING
vsi_nn_graph_t* graphAntiSpf = nullptr;
#endif
int32_t init_neural_networks() {
    const char* model_name_facedetect = nullptr;
    const char* model_name_sface = nullptr;
#if ANTI_SPOOFING
    const char* model_name_anti_spf = nullptr;
#endif
    std::cout << "S/W Version : " << VMX_SW_VERSION << std::endl;
    // OLD FACE DETECTOR - Commented out
    // model_name_facedetect = "network_binary_face_detector.nb";
    // NEW YUNET FACE DETECTOR
    model_name_facedetect = "Yunet.nb";
    // model_name_sface = "sface.export.data";  // Old format
    model_name_sface = "sface.nb";  // New NBG format (optimized)
    // model_name_sface = "facenet512.export.data";
#if ANTI_SPOOFING
    model_name_anti_spf = "anti-spoof-mn3.export.data";
#endif
    std::cout << "model_name_facedetect (YUNET): " << model_name_facedetect << std::endl;
    std::cout << "model_name_sface: " << model_name_sface << std::endl;    
#if ANTI_SPOOFING
    std::cout << "model_name_anti_spf: " << model_name_anti_spf << std::endl;    
#endif
    // OLD FACE DETECTOR - Commented out
    // graphFaceDetect = vnn_CreateNeuralNetworkFacedetector3Uint8(model_name_facedetect);
    // NEW YUNET FACE DETECTOR
    graphFaceDetect = vnn_CreateNeuralNetworkYunet(model_name_facedetect);
    graphSface = vnn_CreateNeuralNetworkSface(model_name_sface);
#if ANTI_SPOOFING
    graphAntiSpf = vnn_CreateNeuralNetworkAntiSpoof(model_name_anti_spf);
#endif
    if (!graphFaceDetect) {
        std::cerr << "Failed to create YUNET face_detector neural network." << std::endl;
        return -1;
    } else if (!graphSface) {
        std::cerr << "Failed to create SFace neural network." << std::endl;
        return -1;
    } 
#if ANTI_SPOOFING
    else if (!graphAntiSpf) {
        std::cerr << "Failed to create AntiSpoof neural network." << std::endl;
        return -1;
    }
#endif
    statusFaceDetect = vnn_VerifyGraph(graphFaceDetect);
    statusSface = vnn_VerifyGraph(graphSface);
#if ANTI_SPOOFING
    statusAntiSpf = vnn_VerifyGraph(graphAntiSpf);
#endif
    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "Graph (YUNET FaceDetect) verification failed." << std::endl;
        // OLD: vnn_ReleaseNeuralNetworkFacedetector3Uint8(graphFaceDetect);
        vnn_ReleaseNeuralNetworkYunet(graphFaceDetect);
        return -1;
    } else if (statusSface != VSI_SUCCESS) {
        std::cerr << "Graph (SFace) verification failed." << std::endl;
        vnn_ReleaseNeuralNetworkSface(graphSface);
        return -1;
    } 
#if ANTI_SPOOFING    
    else if (statusAntiSpf != VSI_SUCCESS) {
        std::cerr << "Graph (AntiSpoof) verification failed." << std::endl;
        vnn_ReleaseNeuralNetworkAntiSpoof(graphAntiSpf);
        return -1;
    }
#endif
    return VSI_SUCCESS;
}

#if 0
cv::Mat resizeKeepAspectRatio(const cv::Mat &input, const cv::Size &dstSize, const cv::Scalar& bgcolor)
{
    cv::Mat output;

    double h1 = dstSize.width * (input.rows/(double)input.cols);
    double w2 = dstSize.height * (input.cols/(double)input.rows);
    
    // initially no borders
    int top = 0;
    int bottom = 0;
    int left = 0;
    int right = 0;
    if (h1 <= dstSize.height)
    {   // Scale based on width
        // only vertical borders
        top = (dstSize.height - h1) / 2;
        bottom = top;
        cv::resize( input, output, cv::Size(dstSize.width, h1));
    } 
    else
    {   // Scale based on height
        // only horizontal borders
        left = (dstSize.width - w2) / 2;
        right = left;
        cv::resize( input, output, cv::Size(w2, dstSize.height));
    }
    cv::copyMakeBorder(output, output, top, bottom, left, right, cv::BORDER_CONSTANT, bgcolor);
    return output;
}
#else
cv::Mat resizeKeepAspectRatio(const cv::Mat &input, const cv::Size &dstSize, const cv::Scalar &bgcolor) {

    cv::Mat output;
    double h_scale = (double)dstSize.height / input.rows;
    double w_scale = (double)dstSize.width / input.cols;
    double scale = std::min(h_scale, w_scale);

    int new_rows = static_cast<int>(input.rows * scale);
    int new_cols = static_cast<int>(input.cols * scale);

    cv::resize(input, output, cv::Size(new_cols, new_rows), 0, 0, cv::INTER_AREA);

    int top = (dstSize.height - new_rows) / 2;
    int bottom = dstSize.height - new_rows - top;
    int left = (dstSize.width - new_cols) / 2;
    int right = dstSize.width - new_cols - left;

    cv::copyMakeBorder(output, output, top, bottom, left, right, cv::BORDER_CONSTANT, bgcolor);
    return output;
}

#endif
std::tuple<uint8_t, std::vector<float>> register_user(cv::Mat img) {
    std::cout << "\n[C++ REG DEBUG] ==> Entered register_user function." << std::endl;
    vsi_status status = VSI_FAILURE;
    std::vector<float> facial_feature = {};

    if (img.empty()) {
        std::cerr << "[C++ REG DEBUG]   ERROR: Input cv::Mat image is empty. Aborting." << std::endl;
        return std::make_tuple(VSI_FAILURE, facial_feature);
    }
    std::cout << "[C++ REG DEBUG]   Input image is valid (not empty)." << std::endl;

    // Yunet expects 640x640 input
    const int YUNET_INPUT_WIDTH = 640;
    const int YUNET_INPUT_HEIGHT = 640;
    cv::Scalar paddingColor(0, 0, 0); // Black padding color (BGR)
    
    std::cout << "[C++ REG DEBUG]   Resizing input image to " << YUNET_INPUT_WIDTH << "x" << YUNET_INPUT_HEIGHT << " with padding." << std::endl;
    cv::Mat yunet_input = resizeKeepAspectRatio(img, cv::Size(YUNET_INPUT_WIDTH, YUNET_INPUT_HEIGHT), paddingColor);
    
    // --- Full Preprocessing Pipeline ---
    std::cout << "[C++ REG DEBUG]   Starting full preprocessing pipeline for Yunet." << std::endl;
    vsi_nn_tensor_t *tensor = vsi_nn_GetTensor( graphFaceDetect, graphFaceDetect->input.tensors[0] );
    if (tensor == nullptr) {
        std::cerr << "[C++ REG DEBUG]   ERROR: Failed to get input tensor for Yunet graph. Aborting." << std::endl;
        return std::make_tuple(VSI_FAILURE, facial_feature);
    }

    _load_input_meta_yunet(); // Ensure metadata is loaded

    // 1. Convert cv::Mat (BGR uint8) to float*
    float *fdata = _imageData_to_float32(yunet_input.data, tensor);
    if (fdata == nullptr) {
        std::cerr << "[C++ REG DEBUG]   ERROR: Failed to convert image data to float32. Aborting." << std::endl;
        return std::make_tuple(VSI_FAILURE, facial_feature);
    }

    // 2. Apply preprocessing steps (reorder, mean, scale)
    for(int i = 0; i < VNN_PREPRO_NUM; i++) // VNN_PREPRO_NUM is 3
    {
        switch (input_meta_tab_yunet[0].image.preprocess[i])
        {
        case VNN_PREPRO_REORDER:
            _data_transform(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        case VNN_PREPRO_MEAN:
            _data_mean(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        case VNN_PREPRO_SCALE:
            _data_scale(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        default:
            break;
        }
    }

    // 3. Convert processed float* data to target dtype (uint8_t*)
    uint8_t *processed_img_buffer = _float32_to_dtype(fdata, tensor);
    if (processed_img_buffer == nullptr) {
        std::cerr << "[C++ REG DEBUG]   ERROR: Failed to convert float32 data to target dtype. Aborting." << std::endl;
        free(fdata); // Clean up float data
        return std::make_tuple(VSI_FAILURE, facial_feature);
    }
    free(fdata); // Clean up float data

    // YUNET PREPROCESSING (now just copies the fully preprocessed buffer)
    std::cout << "[C++ REG DEBUG]   Calling vnn_PreProcessYunet..." << std::endl;
    statusFaceDetect = vnn_PreProcessYunet(graphFaceDetect, 1, processed_img_buffer);

    if (processed_img_buffer) free(processed_img_buffer); // Clean up processed buffer
    // --- End Full Preprocessing Pipeline ---

    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[C++ REG DEBUG]   ERROR: vnn_PreProcessYunet failed with status " << statusFaceDetect << ". Aborting." << std::endl;
        return std::make_tuple(statusFaceDetect, facial_feature);
    }
    std::cout << "[C++ REG DEBUG]   vnn_PreProcessYunet successful." << std::endl;

    std::cout << "[C++ REG DEBUG]   Calling vnn_ProcessGraph for face detection..." << std::endl;
    statusFaceDetect = vnn_ProcessGraph(graphFaceDetect);       
    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[C++ REG DEBUG]   ERROR: vnn_ProcessGraph failed with status " << statusFaceDetect << ". Aborting." << std::endl;
        return std::make_tuple(statusFaceDetect, facial_feature);
    }
    std::cout << "[C++ REG DEBUG]   vnn_ProcessGraph successful." << std::endl;

    // YUNET POSTPROCESSING - extracts faces with landmarks
    std::vector<YunetFaceObject> faces;        
    std::cout << "[C++ REG DEBUG]   Calling vnn_PostProcessNeuralNetworkYunet..." << std::endl;
    statusFaceDetect = vnn_PostProcessNeuralNetworkYunet(graphFaceDetect, faces, YUNET_INPUT_WIDTH, YUNET_INPUT_HEIGHT);       
    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[C++ REG DEBUG]   ERROR: vnn_PostProcessNeuralNetworkYunet failed with status " << statusFaceDetect << ". Aborting." << std::endl;
        return std::make_tuple(statusFaceDetect, facial_feature);
    }
    std::cout << "[C++ REG DEBUG]   vnn_PostProcessNeuralNetworkYunet successful." << std::endl;

    printf("[C++ REG DEBUG]   Number of faces detected: %ld\n", faces.size());
    
    if (faces.size() == 1) {
        std::cout << "[C++ REG DEBUG]   Exactly one face found. Proceeding with feature extraction." << std::endl;
        auto& face = faces[0];
        
        // --- CORRECT SCALING LOGIC ---
        // 1. Recalculate the scale and padding used during resizeKeepAspectRatio
        double h_scale_inv = (double)YUNET_INPUT_HEIGHT / img.rows;
        double w_scale_inv = (double)YUNET_INPUT_WIDTH / img.cols;
        double scale_inv = std::min(h_scale_inv, w_scale_inv);

        int new_scaled_rows = static_cast<int>(img.rows * scale_inv);
        int new_scaled_cols = static_cast<int>(img.cols * scale_inv);

        int pad_top = (YUNET_INPUT_HEIGHT - new_scaled_rows) / 2;
        int pad_left = (YUNET_INPUT_WIDTH - new_scaled_cols) / 2;

        // 2. Convert landmarks back to original image space
        float src_landmarks[5][2];
        for (int i = 0; i < 5; ++i) {
            src_landmarks[i][0] = (face.landmarks[2*i] - pad_left) / scale_inv;
            src_landmarks[i][1] = (face.landmarks[2*i + 1] - pad_top) / scale_inv;
        }
        
        printf("[C++ REG DEBUG]   Scaled landmarks: RE(%.1f,%.1f) LE(%.1f,%.1f) N(%.1f,%.1f) RM(%.1f,%.1f) LM(%.1f,%.1f)\n",
               src_landmarks[0][0], src_landmarks[0][1],
               src_landmarks[1][0], src_landmarks[1][1],
               src_landmarks[2][0], src_landmarks[2][1],
               src_landmarks[3][0], src_landmarks[3][1],
               src_landmarks[4][0], src_landmarks[4][1]);
        
        // üíæ Debug: Draw bbox and landmarks
        {
            cv::Mat debug_img = img.clone();
            
            // Calculate bbox in original image space
            int x1 = (int)((face.rect.x - pad_left) / scale_inv);
            int y1 = (int)((face.rect.y - pad_top) / scale_inv);
            int w = (int)(face.rect.width / scale_inv);
            int h = (int)(face.rect.height / scale_inv);
            
            cv::rectangle(debug_img, cv::Rect(x1, y1, w, h), cv::Scalar(0, 255, 0), 2);
            
            for (int i = 0; i < 5; ++i) {
                cv::circle(debug_img, cv::Point((int)src_landmarks[i][0], (int)src_landmarks[i][1]), 2, cv::Scalar(0, 0, 255), -1);
            }
            
            char debug_path[256];
            time_t now = time(0);
            struct tm* tm_info = localtime(&now);
            strftime(debug_path, sizeof(debug_path), "debug_register_bbox_%Y%m%d_%H%M%S.jpg", tm_info);
            cv::imwrite(debug_path, debug_img);
            printf("[C++ REG DEBUG]   üíæ Saved debug image with bbox/landmarks: %s\n", debug_path);
        }

        std::cout << "[C++ REG DEBUG]   Performing alignCrop..." << std::endl;
        cv::Mat aligned_face;
        // Fix: Create 1x15 matrix to match alignCrop expectation (starts reading at index 4)
        cv::Mat face_mat = cv::Mat::zeros(1, 15, CV_32FC1);
        for (int i = 0; i < 5; i++) {
            face_mat.at<float>(0, 4 + 2 * i) = src_landmarks[i][0];
            face_mat.at<float>(0, 4 + 2 * i + 1) = src_landmarks[i][1];
        }
        
        alignCrop(img, face_mat, aligned_face);
        
        // üíæ Debug: Save aligned face
        char aligned_face_path[256];
        time_t now = time(0);
        struct tm* tm_info = localtime(&now);
        strftime(aligned_face_path, sizeof(aligned_face_path), "debug_register_aligned_%Y%m%d_%H%M%S.jpg", tm_info);
        if (!aligned_face.empty()) {
            cv::imwrite(aligned_face_path, aligned_face);
            printf("[C++ REG DEBUG]   üíæ Saved aligned face (112x112) [INPUT TO SFACE] to: %s\n", aligned_face_path);
        } else {
            std::cerr << "[C++ REG DEBUG]   WARNING: aligned_face is empty after alignCrop." << std::endl;
        }

        std::cout << "[C++ REG DEBUG]   Calling extractFeatures..." << std::endl;
        facial_feature = extractFeatures(aligned_face);
        printf("[C++ REG DEBUG]   <== extractFeatures finished. Extracted feature vector size: %ld\n", facial_feature.size());
        
        if (facial_feature.size() == 128) {
            std::cout << "[C++ REG DEBUG]   SUCCESS: Feature extraction successful. Returning features." << std::endl;
        } else {
            std::cerr << "[C++ REG DEBUG]   ERROR: Feature extraction failed. Vector size is not 128. Returning empty." << std::endl;
            // Ensure we return a failure status if feature extraction fails
            statusFaceDetect = VSI_FAILURE;
        }
        return std::make_tuple(statusFaceDetect, facial_feature);
    }
    else
    {
        std::cerr << "[C++ REG DEBUG]   ERROR: Expected 1 face, but detected " << faces.size() << ". Aborting." << std::endl;
        return std::make_tuple(VSI_FAILURE, facial_feature);
    }
}
std::tuple<uint16_t, uint16_t, uint16_t, uint16_t, float, std::vector<float>> detect_face(cv::Mat img) {

    std::tuple<uint16_t, uint16_t, uint16_t, uint16_t, float, std::vector<float>> empty_result = {0, 0, 0, 0, -1, {}};
    std::vector<float> facial_feature = {};
    float spoof_thresh = 0.4;
    float spoof_confidence = 1;

    // Frame skip check - only process every 4th frame
    if (!shouldProcessFrame()) {
        printf("‚è≠Ô∏è  Skipping frame %d (processing every %d frames)\n", frame_counter, FRAME_SKIP_INTERVAL);
        return empty_result;
    }
    
    printf("üéØ Processing frame %d\n", frame_counter);

    if (img.empty()) {
        std::cout << "Failed to read frame from camera." << std::endl;
        return empty_result;
    }

    // Yunet expects 640x640 input
    const int YUNET_INPUT_WIDTH = 640;
    const int YUNET_INPUT_HEIGHT = 640;
    cv::Scalar paddingColor(0, 0, 0); // Black padding color (BGR)
    
    // Resize image to 640x640 keeping aspect ratio for Yunet
    cv::Mat yunet_input = resizeKeepAspectRatio(img, cv::Size(YUNET_INPUT_WIDTH, YUNET_INPUT_HEIGHT), paddingColor);
    
    // --- Full Preprocessing Pipeline ---
    vsi_nn_tensor_t *tensor = vsi_nn_GetTensor( graphFaceDetect, graphFaceDetect->input.tensors[0] );
    if (tensor == nullptr) {
        std::cerr << "[ERROR] detect_face: Failed to get input tensor for Yunet graph.\n";
        return empty_result;
    }

    _load_input_meta_yunet(); // Ensure metadata is loaded

    // 1. Convert cv::Mat (BGR uint8) to float*
    float *fdata = _imageData_to_float32(yunet_input.data, tensor);
    if (fdata == nullptr) {
        std::cerr << "[ERROR] detect_face: Failed to convert image data to float32.\n";
        return empty_result;
    }

    // 2. Apply preprocessing steps (reorder, mean, scale)
    for(int i = 0; i < VNN_PREPRO_NUM; i++) // VNN_PREPRO_NUM is 3
    {
        switch (input_meta_tab_yunet[0].image.preprocess[i])
        {
        case VNN_PREPRO_REORDER:
            _data_transform(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        case VNN_PREPRO_MEAN:
            _data_mean(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        case VNN_PREPRO_SCALE:
            _data_scale(fdata, &input_meta_tab_yunet[0], tensor);
            break;
        default:
            break;
        }
    }

    // 3. Convert processed float* data to target dtype (uint8_t*)
    uint8_t *processed_img_buffer = _float32_to_dtype(fdata, tensor);
    if (processed_img_buffer == nullptr) {
        std::cerr << "[ERROR] detect_face: Failed to convert float32 data to target dtype.\n";
        free(fdata); // Clean up float data
        return empty_result;
    }
    free(fdata); // Clean up float data

    // YUNET PREPROCESSING (now just copies the fully preprocessed buffer)
    statusFaceDetect = vnn_PreProcessYunet(graphFaceDetect, 1, processed_img_buffer);

    if (processed_img_buffer) free(processed_img_buffer); // Clean up processed buffer
    // --- End Full Preprocessing Pipeline ---

    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[YUNET] Pre-processing failed." << std::endl;
        return empty_result;
    }

    statusFaceDetect = vnn_ProcessGraph(graphFaceDetect);       
    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[YUNET] Graph processing failed." << std::endl;
        return empty_result;
    }

    // YUNET POSTPROCESSING - extracts faces with landmarks
    std::vector<YunetFaceObject> faces;        
    statusFaceDetect = vnn_PostProcessNeuralNetworkYunet(graphFaceDetect, faces, YUNET_INPUT_WIDTH, YUNET_INPUT_HEIGHT);       
    if (statusFaceDetect != VSI_SUCCESS) {
        std::cerr << "[YUNET] Post-processing failed." << std::endl;
        return empty_result;
    }

    printf("[YUNET] The number of faces detected: %ld\n", faces.size());

    if (faces.size() > 0) {
        auto& face = faces[0];  // Get first detected face
        
        // --- CORRECT SCALING LOGIC ---
        // 1. Recalculate the scale and padding used during resizeKeepAspectRatio
        double h_scale_inv = (double)YUNET_INPUT_HEIGHT / img.rows;
        double w_scale_inv = (double)YUNET_INPUT_WIDTH / img.cols;
        double scale_inv = std::min(h_scale_inv, w_scale_inv);

        int new_scaled_rows = static_cast<int>(img.rows * scale_inv);
        int new_scaled_cols = static_cast<int>(img.cols * scale_inv);

        int pad_top = (YUNET_INPUT_HEIGHT - new_scaled_rows) / 2;
        int pad_left = (YUNET_INPUT_WIDTH - new_scaled_cols) / 2;

        // 2. Convert face coordinates from 640x640 space back to original image space
        uint16_t left, top, right, bottom;
        left = (uint16_t)std::max(0.0, (face.rect.x - pad_left) / scale_inv);
        top = (uint16_t)std::max(0.0, (face.rect.y - pad_top) / scale_inv);
        right = (uint16_t)std::min((double)img.cols, ((face.rect.x + face.rect.width) - pad_left) / scale_inv);
        bottom = (uint16_t)std::min((double)img.rows, ((face.rect.y + face.rect.height) - pad_top) / scale_inv);

        // 3. Convert landmarks back to original image space
        float src_landmarks[5][2];
        for (int i = 0; i < 5; ++i) {
            src_landmarks[i][0] = (face.landmarks[2*i] - pad_left) / scale_inv;
            src_landmarks[i][1] = (face.landmarks[2*i + 1] - pad_top) / scale_inv;
        }
        
        printf("[YUNET] Scaled landmarks: RE(%.1f,%.1f) LE(%.1f,%.1f) N(%.1f,%.1f) RM(%.1f,%.1f) LM(%.1f,%.1f)\n",
               src_landmarks[0][0], src_landmarks[0][1],
               src_landmarks[1][0], src_landmarks[1][1],
               src_landmarks[2][0], src_landmarks[2][1],
               src_landmarks[3][0], src_landmarks[3][1],
               src_landmarks[4][0], src_landmarks[4][1]);

        // üíæ Debug: Draw bbox and landmarks
        /*
        {
            cv::Mat debug_img = img.clone();
            cv::rectangle(debug_img, cv::Point(left, top), cv::Point(right, bottom), cv::Scalar(0, 255, 0), 2);
            
            for (int i = 0; i < 5; ++i) {
                cv::circle(debug_img, cv::Point((int)src_landmarks[i][0], (int)src_landmarks[i][1]), 2, cv::Scalar(0, 0, 255), -1);
            }
            
            char debug_path[256];
            time_t now = time(0);
            struct tm* tm_info = localtime(&now);
            strftime(debug_path, sizeof(debug_path), "debug_detect_bbox_%Y%m%d_%H%M%S.jpg", tm_info);
            cv::imwrite(debug_path, debug_img);
            printf("üíæ [YUNET] Saved debug image with bbox/landmarks: %s\n", debug_path);
        }
        */

        // ‚úÖ USE ALIGNCROP WITH YUNET LANDMARKS for better face alignment
        cv::Mat aligned_face;
        // Fix: Create 1x15 matrix to match alignCrop expectation (starts reading at index 4)
        cv::Mat face_mat = cv::Mat::zeros(1, 15, CV_32FC1);
        for (int i = 0; i < 5; i++) {
            face_mat.at<float>(0, 4 + 2 * i) = src_landmarks[i][0];
            face_mat.at<float>(0, 4 + 2 * i + 1) = src_landmarks[i][1];
        }
        
        alignCrop(img, face_mat, aligned_face);
        
        // Debug: Save aligned face
        /*
        char aligned_face_path[256];
        time_t now = time(0);
        struct tm* tm_info = localtime(&now);
        strftime(aligned_face_path, sizeof(aligned_face_path), "debug_detect_aligned_%Y%m%d_%H%M%S.jpg", tm_info);
        if (!aligned_face.empty()) {
            cv::imwrite(aligned_face_path, aligned_face);
            printf("üíæ [YUNET] Saved aligned face [INPUT TO SFACE]: %s\n", aligned_face_path);
        }
        */

        // Extract facial features from aligned face (112x112 - already done by alignCrop)
        facial_feature = extractFeatures(aligned_face);
        printf("[Sface] Extracted facial feature vector, size: %ld\n", facial_feature.size());
        
    #if ANTI_SPOOFING
        // Use bounding box from original image for anti-spoofing
        int x1 = std::max(0, std::min((int)left, img.cols - 1));
        int y1 = std::max(0, std::min((int)top, img.rows - 1));
        int x2 = std::max(0, std::min((int)right, img.cols - 1));
        int y2 = std::max(0, std::min((int)bottom, img.rows - 1));
        int w = x2 - x1;
        int h = y2 - y1;
        
        if (w > 0 && h > 0) {
            cv::Rect b_box(x1, y1, w, h);
            cv::Mat anti_spf_input = img(b_box);
            
            anti_spf_input = resizeKeepAspectRatio(anti_spf_input, cv::Size(128, 128), paddingColor);
            anti_spf_input.convertTo(anti_spf_input, CV_32FC3, 1.0 / 255.0f);
            cv::Scalar mean(0.485, 0.456, 0.406);
            cv::Scalar std(0.229, 0.224, 0.225);
            anti_spf_input = (anti_spf_input - mean) / std;
            spoof_confidence = checkSpoofing(anti_spf_input);
            printf("[YUNET] Anti-spoofing confidence: %f\n", spoof_confidence);

            // ‚úÖ FACE RECOGNITION & ACCESS LOGGING: Only if face is real
            if (spoof_confidence > spoof_thresh && !facial_feature.empty()) {
                printf("=== FACE RECOGNITION SEARCH ===\n");
                // Note: Add face database search here if available
                std::string dummy_username = "TestUser_" + std::to_string(std::rand() % 100);
                float dummy_similarity = 0.75f + (std::rand() % 25) / 100.0f;
                
                printf("‚úÖ FACE RECOGNIZED: %s (confidence: %.4f)\n", dummy_username.c_str(), dummy_similarity);
                
                // ‚úÖ ACCESS LOGGING: Check if we should log this access
                if (shouldLogAccess(dummy_username)) {
                    logAccess(dummy_username, dummy_similarity);
                    printf("üìù Access logged for user: %s\n", dummy_username.c_str());
                } else {
                    printf("‚è∞ Access not logged (too soon since last access for %s)\n", dummy_username.c_str());
                }
                printf("===============================\n");
            }
        }
    #else
        // No anti-spoofing, just return results
        printf("[YUNET] No anti-spoofing enabled\n");
    #endif
        return std::make_tuple(left, top, right, bottom, spoof_confidence, facial_feature);

    } else {
        std::cerr << "[YUNET] Please capture person FACE." << std::endl;
        return empty_result;
    }
}

std::vector<float> extractFeatures(cv::Mat img) {

    vsi_status status = VSI_FAILURE;
    uint8_t *img_buffer = NULL;
    // uint32_t *img_buffer_size = 0;
    std::vector<float> feature;
    
    //Th·ª≠ nghi·ªám c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c b·∫±ng c√°ch chuy·ªÉn BGR sang RGB
    //cv::cvtColor(img, img, cv::COLOR_BGR2RGB);
    // img_buffer = pre_process_get_buffer_from_mat(img, img_buffer_size);
    if (img.isContinuous())
        img_buffer = img.data;

    /* Pre process the image data */
    // uint32_t input_num = 1;
    // printf("[%s()]::[%d] graphSface->input.num = %d\n", __FUNCTION__, __LINE__, graphSface->input.num);
    //status = vnn_PreProcessNeuralNetwork( graphSface, 1, img_buffer );
    status = vnn_PreProcess( graphSface, 1, img_buffer );
    TEST_CHECK_STATUS( status, final );

    /* Process graph */
    status = vnn_ProcessGraph( graphSface );
    TEST_CHECK_STATUS( status, final );

    if(VNN_APP_DEBUG)
    {
        /* Dump all node outputs */
        vsi_nn_DumpGraphNodeOutputs(graphSface, "./network_dump", NULL, 0, TRUE, 0);
    }

    /* Post process output data */
    // status = vnn_PostProcessNeuralNetworkSface( graphSface );
    // TEST_CHECK_STATUS( status, final );
    feature = vnn_PostProcessSfaceExtractFeature( graphSface );

final:
    // vnn_ReleaseNeuralNetworkSface( graphSface );
    fflush(stdout);
    fflush(stderr);
    // return status;
    return feature;

}
#if ANTI_SPOOFING
float checkSpoofing(cv::Mat img) {
    vsi_status status = VSI_FAILURE;
    uint8_t *img_buffer = NULL;
    // uint32_t *img_buffer_size = 0;

    
    // img_buffer = pre_process_get_buffer_from_mat(img, img_buffer_size);
    if (img.isContinuous())
        img_buffer = img.data;

    /* Pre process the image data */
    // uint32_t input_num = 1;
    // printf("[%s()]::[%d] graphAntiSpf->input.num = %d\n", __FUNCTION__, __LINE__, graphAntiSpf->input.num);
    status = vnn_PreProcessNeuralNetwork( graphAntiSpf, 1, img_buffer );
    TEST_CHECK_STATUS( status, final );

    /* Process graph */
    status = vnn_ProcessGraph( graphAntiSpf );
    TEST_CHECK_STATUS( status, final );

    if(VNN_APP_DEBUG)
    {
        /* Dump all node outputs */
        vsi_nn_DumpGraphNodeOutputs(graphAntiSpf, "./network_dump", NULL, 0, TRUE, 0);
    }

    /* Post process output data */
    // TEST_CHECK_STATUS( status, final );

    return vnn_PostProcessNeuralNetworkAntiSpoof(graphAntiSpf);

final:
    // vnn_ReleaseNeuralNetworkAntiSpoof( graphAntiSpf );
    fflush(stdout);
    fflush(stderr);
    return -1;
}
#endif
